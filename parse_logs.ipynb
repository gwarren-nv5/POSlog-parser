{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POSPac Logfile parser:\n",
    "\n",
    "Default file location @ Z:\\RESOURCES\\Production\\Bridge\\02_Logfiles\\01_POS_Processing\n",
    "\n",
    "Created by GWarren on 8/2/2023  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder path for bulk log files\n",
    "folder_path = r\"Z:\\RESOURCES\\Production\\Bridge\\02_Logfiles\\01_POS_Processing\"\n",
    "\n",
    "# Apply optional date range\n",
    "# Write None for no range, write 'YYMMDD' in quotes for timerange\n",
    "START_DATE = None  # '210408' or NONE\n",
    "END_DATE = None    # '210410' or NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the lines into key-value pairs considering the dots in values\n",
    "def parse_line_corrected(line):\n",
    "    key_parts = line.split('.')\n",
    "    key = key_parts[0].strip()\n",
    "    value = '.'.join(key_parts[1:]).lstrip('.').strip() if len(key_parts) > 1 else None\n",
    "    return key, value\n",
    "\n",
    "# Exclusion list\n",
    "exclude_keys = [\"MoveOut Settings\", \"POF Import Settings\", \"MESSAGES\", \"METRICS\"]\n",
    "\n",
    "# Function to check if a line should be excluded\n",
    "def should_exclude(line):\n",
    "    return line.strip() in exclude_keys or line.strip() == ''\n",
    "\n",
    "def parse_line_final(line):\n",
    "    if \"Processing started for\" in line:\n",
    "        return \"Successful run\", \"POS processing succeeded!\" in line\n",
    "    else:\n",
    "        return parse_line_corrected(line)\n",
    "    \n",
    "def convert_to_MB(value):\n",
    "    if \"GB\" in str(value):\n",
    "        return float(value.replace(\"GB\", \"\").strip()) * 1024\n",
    "    elif \"MB\" in str(value):\n",
    "        return float(value.replace(\"MB\", \"\").strip())\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def extract_minutes(value):\n",
    "    parts = str(value).split()\n",
    "    return float(parts[0]) if parts else value\n",
    "\n",
    "def convert_to_seconds(value):\n",
    "    if \"minutes\" in str(value):\n",
    "        return float(value.replace(\"minutes\", \"\").strip()) * 60\n",
    "    elif \"hours\" in str(value):\n",
    "        return float(value.replace(\"hours\", \"\").strip()) * 3600\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def process_file(file_path):\n",
    "    key_value_pairs_final = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if not should_exclude(line):\n",
    "                key, value = parse_line_final(line.strip())\n",
    "                key_value_pairs_final.append((key, value))\n",
    "    df = pd.DataFrame(key_value_pairs_final, columns=['Key', 'Value'])\n",
    "    df_transposed = df.set_index('Key').transpose()\n",
    "    return df_transposed\n",
    "\n",
    "def process_folder(folder_path, start_date=None, end_date=None):\n",
    "    all_dataframes = []\n",
    "    \n",
    "    # Convert start_date and end_date to datetime objects if they're not None\n",
    "    start_date = datetime.strptime(start_date, \"%y%m%d\") if start_date else None\n",
    "    end_date = datetime.strptime(end_date, \"%y%m%d\") if end_date else None\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Use regex to match the first six digits (the date) and ignore following characters\n",
    "            date_match = re.match(r\"(\\d{6})\", filename)\n",
    "            if date_match:\n",
    "                file_date = datetime.strptime(date_match.group(), \"%y%m%d\")\n",
    "\n",
    "                # Check if the file_date is within the range\n",
    "                if ((not start_date or file_date >= start_date) and (not end_date or file_date <= end_date)):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    df_transposed = process_file(file_path)\n",
    "                    all_dataframes.append(df_transposed)\n",
    "    final_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Applying the conversion to the \"SBET Size\" column\n",
    "    final_df['SBET Size'] = final_df['SBET Size'].apply(convert_to_MB)\n",
    "\n",
    "    # Cleaning up the \"Processing Rate\" column\n",
    "    final_df['Processing Rate'] = final_df['Processing Rate'].apply(extract_minutes)\n",
    "    final_df.rename(columns={'Processing Rate': 'Processing Rate (min/SN hr)'}, inplace=True)\n",
    "    \n",
    "    # Converting specified columns to seconds and updating the column names\n",
    "    columns_to_convert = ['SBET Extract Time', 'HTDP Transform Time', 'POF Import Time', 'Copy Time', 'Total Proc Time', 'Flight Duration']\n",
    "    for col in columns_to_convert:\n",
    "        final_df[col] = final_df[col].apply(convert_to_seconds)\n",
    "        final_df.rename(columns={col: col + ' (sec)'}, inplace=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "df_final = process_folder(folder_path, START_DATE, END_DATE)\n",
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a scatter plot for \"SBET Size\" vs \"Copy Time\" with a trend line\n",
    "correlation = df_final['SBET Size'].corr(df_final['Copy Time (sec)'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_final['SBET Size'], df_final['Copy Time (sec)'])\n",
    "plt.plot(df_final['SBET Size'], np.poly1d(np.polyfit(df_final['SBET Size'], df_final['Copy Time (sec)'], 1))(df_final['SBET Size']), color='red')\n",
    "plt.xlabel('SBET Size (MB)')\n",
    "plt.ylabel('Copy Time (seconds)')\n",
    "plt.title('Scatter Plot of \"SBET Size\" vs \"Copy Time\"')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Copy Time by machine\n",
    "average_copy_time_by_run_from = df_final.groupby('Run from')['Copy Time (sec)'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plotting a bar chart for average \"Copy Time\" by \"Run from\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "average_copy_time_by_run_from.plot(kind='bar')\n",
    "plt.ylabel('Average Copy Time (minutes)')\n",
    "plt.title('Average Copy Time by \"Run from\" Device')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Avg Proc Rate by Device, with outliers removed\n",
    "\n",
    "# Calculate the mean and standard deviation\n",
    "mean_processing_rate = df_final['Processing Rate (min/SN hr)'].mean()\n",
    "std_processing_rate = df_final['Processing Rate (min/SN hr)'].std()\n",
    "\n",
    "# Define a threshold for excluding extreme values (4 standard deviations from the mean)\n",
    "threshold = 4 * std_processing_rate\n",
    "\n",
    "# Create a filter for extreme values\n",
    "extreme_values_filter = abs(df_final['Processing Rate (min/SN hr)'] - mean_processing_rate) > threshold\n",
    "\n",
    "# Filter the DataFrame to exclude extreme values\n",
    "filtered_df_final = df_final[~extreme_values_filter]\n",
    "\n",
    "# Get the extreme (removed) values, print\n",
    "removed_values = df_final[extreme_values_filter]['Processing Rate (min/SN hr)']\n",
    "print(\"Removed values:\")\n",
    "print(removed_values.tolist())\n",
    "\n",
    "# Grouping by \"Run from\" and calculating the average \"Processing Rate (min/SN hr)\"\n",
    "average_processing_rate_by_run_from = filtered_df_final.groupby('Run from')['Processing Rate (min/SN hr)'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plotting a bar chart for average \"Processing Rate (min/SN hr)\" by \"Run from\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "average_processing_rate_by_run_from.plot(kind='bar')\n",
    "plt.ylabel('Average Processing Rate (min/SN hr)')\n",
    "plt.title('Average Processing Rate (min/SN hr) by Device')\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of jobs on each machine\n",
    "job_count_by_run_from = df_final['Run from'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Plotting a bar chart for the count of jobs by \"Run from\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "job_count_by_run_from.plot(kind='bar')\n",
    "plt.ylabel('Count of Jobs')\n",
    "plt.title('Count of Jobs by \"Run from\" Device')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "job_count_by_run_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get count of processing method, geoid, and epoch date\n",
    "proc_mode_counts = df_final['Proc Mode'].value_counts()\n",
    "print(\"Processing Modes: \\n\",proc_mode_counts)\n",
    "\n",
    "geoid_counts = df_final['Geoid'].value_counts()\n",
    "print(\"\\nGeoids: \\n\",geoid_counts)\n",
    "\n",
    "target_counts = df_final['Target Date'].value_counts()\n",
    "print(\"\\nTarget Epoch: \\n\",target_counts)\n",
    "\n",
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anomaly Detection w. SKlearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = df_final[['Total Proc Time (sec)', 'SBET Size']]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = IsolationForest(contamination=0.05) # Adjust contamination as needed\n",
    "model.fit(scaled_features)\n",
    "\n",
    "anomaly_scores = model.decision_function(scaled_features)\n",
    "anomalies = model.predict(scaled_features)\n",
    "\n",
    "plt.scatter(features['Total Proc Time (sec)'], features['SBET Size'], c=anomalies)\n",
    "plt.xlabel('Total Proc Time (sec)')\n",
    "plt.ylabel('SBET Size')\n",
    "plt.title('Anomaly Detection')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
